{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from scrape_liferay import authenticate, session\n",
    "from subprocess import Popen, PIPE\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from subprocess import DEVNULL\n",
    "except ImportError:\n",
    "    import os\n",
    "    DEVNULL = open(os.devnull, 'wb')\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `test_folders` to `None` if no longer testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folders = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Git CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _git(*args, stderr=PIPE):\n",
    "    cmd = ['git'] + list(args)\n",
    "\n",
    "    if args[0] != 'config':\n",
    "        logging.info(' '.join(cmd))\n",
    "\n",
    "    pipe = Popen(cmd, stdout=PIPE, stderr=stderr)\n",
    "    out, err = pipe.communicate()\n",
    "\n",
    "    return out.decode('UTF-8', 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GitHubRepository = namedtuple(\n",
    "    'GitHubRepository',\n",
    "    ' '.join(['git_root', 'origin', 'upstream', 'branch', 'project_folder', 'single_folder'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrowdInRepository = namedtuple(\n",
    "    'CrowdinRepository',\n",
    "    ' '.join(['project_id', 'api_key', 'dest_folder'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TranslationRepository = namedtuple(\n",
    "    'TranslationRepository',\n",
    "    ' '.join(['github', 'crowdin'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repository(git_repository, git_branch, git_folder, project_id, project_folder, single_folder):\n",
    "    single_folder = single_folder.strip()\n",
    "\n",
    "    if len(single_folder) == 0:\n",
    "        single_folder = None\n",
    "    else:\n",
    "        single_folder = git_folder + '/' + single_folder\n",
    "    \n",
    "    git_root = os.path.dirname(initial_dir) + '/' + git_repository\n",
    "\n",
    "    os.chdir(git_root)\n",
    "\n",
    "    logging.info(git_root)\n",
    "\n",
    "    origin_url = _git('remote', 'get-url', 'origin').strip()\n",
    "    origin = origin_url.split(':')[1][:-4]\n",
    "\n",
    "    upstream_url = _git('remote', 'get-url', 'upstream').strip()\n",
    "    \n",
    "    if len(upstream_url) == 0:\n",
    "        upstream = None\n",
    "    else:\n",
    "        upstream = upstream_url.split(':')[1][:-4]\n",
    "\n",
    "    project_api_key = _git('config', 'crowdin.api-key.%s' % project_id).strip()\n",
    "\n",
    "    os.chdir(initial_dir)\n",
    "\n",
    "    return TranslationRepository(\n",
    "        GitHubRepository(git_root, origin, upstream, git_branch, git_folder, single_folder),\n",
    "        CrowdInRepository(project_id, project_api_key, project_folder)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(initial_dir)\n",
    "repositories_df = pd.read_csv('repositories.csv')\n",
    "repositories_df.fillna('', inplace=True)\n",
    "repositories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repositories = [get_repository(**x) for x in repositories_df.to_dict('records')]\n",
    "repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GitHub API Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an OAuth token, a base URL, and an API path, we make requests against GitHub by using the `requests` module, and we interpret the response as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_oauth_token = _git('config', 'github.oauth-token').strip()\n",
    "assert(github_oauth_token is not None)\n",
    "\n",
    "github_base_url = 'https://api.github.com'\n",
    "\n",
    "def github_request(api_path, request_type=None, data=None):\n",
    "    headers = {\n",
    "        'user-agent': 'python',\n",
    "        'authorization': 'token %s' % github_oauth_token,\n",
    "        'accept': 'application/vnd.github.inertia-preview+json'\n",
    "    }\n",
    "\n",
    "    logging.info('github-api %s' % api_path)\n",
    "\n",
    "    if data is None:\n",
    "        r = requests.get(github_base_url + api_path, headers=headers)\n",
    "    elif request_type == 'PATCH':\n",
    "        r = requests.patch(github_base_url + api_path, json=data, headers=headers)\n",
    "    elif request_type == 'POST':\n",
    "        r = requests.post(github_base_url + api_path, json=data, headers=headers)\n",
    "\n",
    "    if r.status_code < 200 or r.status_code >= 400:\n",
    "        logging.error('HTTP Error: %d' % r.status_code)\n",
    "        return (r.status_code, r.headers, None)\n",
    "\n",
    "    return (r.status_code, r.headers, r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the general privacy restrictions, the GitHub API also adds a rate limit to the REST API endpoints. We'll want to make sure that any request we send to GitHub is aware of those limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global remaining\n",
    "\n",
    "remaining = 0\n",
    "\n",
    "def wait_for_rate_limit_reset():\n",
    "    global remaining\n",
    "\n",
    "    if remaining > 0:\n",
    "        remaining -= 1\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        status_code, headers, result = github_request('/rate_limit')\n",
    "\n",
    "        resources = result['resources']['core']\n",
    "        remaining = resources['remaining']\n",
    "\n",
    "        if remaining > 0:\n",
    "            remaining -= 1\n",
    "            return\n",
    "\n",
    "        wait_time = 1 + int(resources['reset'] - datetime.now().timestamp())\n",
    "\n",
    "        logging.error('Waiting %d seconds for rate limit reset' % wait_time)\n",
    "\n",
    "        time.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to waiting for the rate limit to reset, we also need to think about what happens if we make multiple requests against the API.\n",
    "\n",
    "For example, let's say we had an hourly script that routinely crawled all the forks of the `liferay-portal` repository checking for new pull requests or new pull request comments. When we do this, we'll also want to make sure that when we make a request to GitHub, that we exclude data that overlaps with data we've already retrieved. This filtering method both waits for the rate limit, and then filters the results, after making the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_github_request(api_path, request_type=None, data=None, field_name=None, min_field_value=None):\n",
    "    global remaining\n",
    "\n",
    "    if remaining == 0:\n",
    "        wait_for_rate_limit_reset()\n",
    "\n",
    "    status_code, headers, result = github_request(api_path, request_type, data)\n",
    "\n",
    "    if result is None:\n",
    "        remaining = int(headers['X-RateLimit-Remaining']) if 'X-RateLimit-Remaining' in headers else 0\n",
    "\n",
    "        if remaining == 0:\n",
    "            wait_for_rate_limit_reset()\n",
    "            status_code, headers, result = github_request(api_path, request_type, data)\n",
    "\n",
    "    if result is None:\n",
    "        return (status_code, headers, [])\n",
    "\n",
    "    if min_field_value is None:\n",
    "        return (status_code, headers, result)\n",
    "\n",
    "    return (status_code, headers, [item for item in result if item[field_name] > min_field_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_github_request_all(api_path, request_type=None, data=None, field_name=None, min_field_value=None):\n",
    "    results = []\n",
    "\n",
    "    while api_path is not None:\n",
    "        status_code, headers, new_results = filter_github_request(\n",
    "            api_path, request_type, data, field_name, min_field_value)\n",
    "\n",
    "        results.extend(new_results)\n",
    "\n",
    "        lower_headers = {\n",
    "            key.lower(): value for key, value in headers.items()\n",
    "        }\n",
    "\n",
    "        if 'link' not in lower_headers:\n",
    "            break\n",
    "\n",
    "        api_path = None\n",
    "\n",
    "        for link in lower_headers['link'].split(','):\n",
    "            url_info, rel_info = [info.strip() for info in link.split(';')]\n",
    "\n",
    "            rel = rel_info.split('\"')[1]\n",
    "            if rel == 'next':\n",
    "                api_path = url_info[len('<https://api.github.com'):-1]\n",
    "\n",
    "    return status_code, headers, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll want to make sure that our OAuth token actually works and can visit the GitHub repositories we've defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_repository_accessible(reviewer_url):\n",
    "    if reviewer_url is None:\n",
    "        return True\n",
    "    \n",
    "    api_path = '/repos/%s' % reviewer_url\n",
    "\n",
    "    status_code, headers, result = filter_github_request(api_path)\n",
    "\n",
    "    return result is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for git_repository, crowdin_repository in repositories:\n",
    "    assert(is_repository_accessible(git_repository.origin))\n",
    "    assert(is_repository_accessible(git_repository.upstream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrowdIn CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crowdin(*args, stderr=PIPE):\n",
    "    cmd = ['crowdin'] + list(args)\n",
    "\n",
    "    logging.info(' '.join(cmd))\n",
    "\n",
    "    pipe = Popen(cmd, stdout=PIPE, stderr=stderr)\n",
    "    out, err = pipe.communicate()\n",
    "\n",
    "    return out.decode('UTF-8', 'replace').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pandoc` to disable word wrapping to improve machine translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pandoc(filename, *args):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        title_pos = -1\n",
    "        toc_pos = -1\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            if title_pos == -1 and line.find('#') == 0:\n",
    "                title_pos = i\n",
    "            elif line.find('[TOC') == 0:\n",
    "                toc_pos = i\n",
    "\n",
    "        head_lines = ''.join(lines[0:max(title_pos, toc_pos)+1])\n",
    "        tail_lines = ''.join(lines[max(title_pos, toc_pos)+1:])\n",
    "\n",
    "    cmd = ['pandoc'] + list(args)\n",
    "\n",
    "    pipe = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
    "    out, err = pipe.communicate(input=tail_lines.encode('UTF-8'))\n",
    "\n",
    "    nowrap_lines = out.decode('UTF-8', 'replace')\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(head_lines)\n",
    "        f.write('\\n')\n",
    "        f.write(nowrap_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a `crowdin.yaml` file to tell the CLI what to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crowdin_file(repository, local_file):\n",
    "    return repository.crowdin.dest_folder + '/' + local_file[len(repository.github.project_folder)+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_file(repository, crowdin_file):\n",
    "    return repository.github.project_folder + crowdin_file[len(repository.crowdin.dest_folder):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crowdin_config_entry(repository, file):\n",
    "    assert(not os.path.isdir(file))\n",
    "\n",
    "    if file[0:3] == 'en/':\n",
    "        translation = '%two_letters_code%/' + file[3:]\n",
    "    else:\n",
    "        translation = file.replace('/en/', '/%two_letters_code%/')\n",
    "\n",
    "    dest = '/' + get_crowdin_file(repository, file)\n",
    "\n",
    "    return {\n",
    "        'source': file,\n",
    "        'dest': dest,\n",
    "        'translation': translation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_crowdin(repository, files):\n",
    "    configs = [get_crowdin_config_entry(repository, file) for file in files]\n",
    "    config_json = json.dumps(configs, indent=2)\n",
    "\n",
    "    with open('%s/crowdin.yaml' % repository.github.git_root, 'w') as f:\n",
    "        f.write('''\n",
    "\"project_identifier\" : \"{crowdin_project_id}\"\n",
    "\"api_key\" : \"{crowdin_api_key}\"\n",
    "\"base_path\" : \"{git_root}\"\n",
    "\"preserve_hierarchy\": true\n",
    "\n",
    "\"files\": {files}\n",
    "'''.format(\n",
    "        crowdin_project_id=repository.crowdin.project_id,\n",
    "        crowdin_api_key=repository.crowdin.api_key,\n",
    "        git_root=repository.github.git_root,\n",
    "        files=config_json\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be able to convert a list of files into a list of root folders, since we want to break everything up at the root folder level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root_folders(repository, candidate_files):\n",
    "    candidate_folders = get_folders(candidate_files)\n",
    "\n",
    "    root_folders = set()\n",
    "    \n",
    "    prefix = repository.github.project_folder + '/'\n",
    "\n",
    "    single_folder = repository.github.single_folder\n",
    "    \n",
    "    if single_folder is None:\n",
    "        prefix = repository.github.project_folder + '/'\n",
    "    else:\n",
    "        prefix = repository.github.single_folder + '/'\n",
    "    \n",
    "    for folder in candidate_folders:\n",
    "        if folder == repository.github.project_folder:\n",
    "            continue\n",
    "\n",
    "        if folder != single_folder and folder.find(prefix) != 0:\n",
    "            continue\n",
    "\n",
    "        parent_folder = folder\n",
    "\n",
    "        while parent_folder != repository.github.project_folder:\n",
    "            folder = parent_folder\n",
    "            parent_folder = os.path.dirname(folder)\n",
    "\n",
    "        if parent_folder != '':\n",
    "            root_folders.add(folder)\n",
    "\n",
    "    return list(root_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper functions to upload sources and download translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowdin_upload_sources(repository, new_files):\n",
    "    before_upload = get_crowdin_file_info(repository)\n",
    "\n",
    "    for file in new_files:\n",
    "        extension = file[file.rfind('.'):]\n",
    "\n",
    "        if extension == '.md' or extension == '.markdown':\n",
    "            _pandoc(file, '--from=gfm', '--to=gfm', '--wrap=none')\n",
    "\n",
    "    if len(new_files) > 0:\n",
    "        root_folders = get_root_folders(repository, new_files)\n",
    "\n",
    "        for root_folder in root_folders:\n",
    "            prefix = root_folder + '/'\n",
    "            folder_files = [file for file in new_files if file.find(prefix) == 0]\n",
    "            \n",
    "            configure_crowdin(repository, folder_files)\n",
    "            _crowdin('upload', 'sources')\n",
    "\n",
    "    _git('reset', '--hard')\n",
    "\n",
    "    after_upload = get_crowdin_file_info(repository)\n",
    "\n",
    "    return before_upload, after_upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowdin_download_translations(repository, all_files, new_files, file_info):\n",
    "    updated_files = list(new_files)\n",
    "\n",
    "    for file in set(all_files).difference(set(new_files)):\n",
    "        crowdin_file = get_crowdin_file(repository, file)\n",
    "\n",
    "        if crowdin_file not in file_info:\n",
    "            continue\n",
    "\n",
    "        metadata = file_info[crowdin_file]\n",
    "\n",
    "        if metadata['phrases'] == metadata['approved']:\n",
    "            updated_files.append(file)\n",
    "            continue\n",
    "\n",
    "        target_file = 'ja/' + file[3:] if file[0:3] == 'en/' else file.replace('/en/', '/ja/')\n",
    "\n",
    "        if not os.path.isfile(target_file):\n",
    "            updated_files.append(file)\n",
    "            continue\n",
    "\n",
    "    if len(updated_files) > 0:\n",
    "        configure_crowdin(repository, updated_files)\n",
    "\n",
    "        _crowdin('download', '-l', 'ja')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrowdIn API Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crowdin_base_url = 'https://api.crowdin.com/api/project/'\n",
    "\n",
    "def crowdin_request(repository, api_path, request_type='GET', data=None, files=None):\n",
    "    headers = {\n",
    "        'user-agent': 'python'\n",
    "    }\n",
    "\n",
    "    request_url = crowdin_base_url + repository.crowdin.project_id + api_path + \\\n",
    "        '?key=' + repository.crowdin.api_key\n",
    "\n",
    "    if request_type == 'GET':\n",
    "        r = requests.get(request_url, data=data, headers=headers)\n",
    "    else:\n",
    "        r = requests.post(request_url, data=data, files=files, headers=headers)\n",
    "\n",
    "    if r.status_code < 200 or r.status_code >= 400:\n",
    "        logging.error('HTTP Error: %d' % r.status_code)\n",
    "        return (r.status_code, None)\n",
    "\n",
    "    return (r.status_code, r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crowdin File Management API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API calls to download the translation memory and glossary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_translation_memory(repository):\n",
    "    logging.info('crowdin-api download-tm')\n",
    "\n",
    "    data = {\n",
    "        'source_language': 'en',\n",
    "        'target_language': 'ja'\n",
    "    }\n",
    "\n",
    "    status_code, response_text = crowdin_request(repository, '/download-tm', 'GET', data)\n",
    "\n",
    "    if response_text is not None:\n",
    "        with open('%s/%s.tmx' % (initial_dir, repository.crowdin.project_id), 'w') as f:\n",
    "            f.write(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_glossary(repository):\n",
    "    logging.info('crowdin-api download-glossary')\n",
    "\n",
    "    status_code, response_text = crowdin_request(repository, '/download-glossary', 'GET')\n",
    "\n",
    "    if response_text is not None:\n",
    "        with open('%s/%s.tbx' % (initial_dir, repository.crowdin.project_id), 'w') as f:\n",
    "            f.write(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API call to delete existing translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_translation(repository, file):\n",
    "    logging.info('crowdin-api delete-file %s' % file)\n",
    "\n",
    "    data = {\n",
    "        'file': file\n",
    "    }\n",
    "\n",
    "    return crowdin_request(repository, '/delete-file', 'POST', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API calls to find the file IDs for CrowdIn (used to generate links within the issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_crowdin_file_info(files_element, current_path, file_info):\n",
    "    for item in files_element.children:\n",
    "        if item.name != 'item':\n",
    "            continue\n",
    "\n",
    "        item_name = item.find('name').text\n",
    "        item_node_type = item.find('node_type').text\n",
    "\n",
    "        item_path = current_path + '/' + item_name if current_path is not None else item_name\n",
    "\n",
    "        file_info[item_path] = {\n",
    "            'phrases': int(item.find('phrases').text),\n",
    "            'translated': int(item.find('translated').text),\n",
    "            'approved': int(item.find('approved').text)\n",
    "        }\n",
    "\n",
    "        if item_node_type == 'file':\n",
    "            file_info[item_path]['id'] = item.find('id').text\n",
    "        else:\n",
    "            extract_crowdin_file_info(item.find('files'), item_path, file_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crowdin_file_info(repository):\n",
    "    logging.info('crowdin-api language-status')\n",
    "\n",
    "    data = {\n",
    "        'language': 'ja'\n",
    "    }\n",
    "\n",
    "    status_code, response_text = crowdin_request(\n",
    "        repository, '/language-status', 'POST', data)\n",
    "\n",
    "    file_info = {}\n",
    "\n",
    "    if response_text is not None:\n",
    "        soup = BeautifulSoup(response_text, features='html.parser')\n",
    "        extract_crowdin_file_info(soup.find('files'), None, file_info)\n",
    "\n",
    "    return file_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crowdin Translation API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send requests to CrowdIn to do automated translation (translation memory, then machine translation, then translation memory again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_with_memory(repository, files):\n",
    "    data = {\n",
    "        'languages[]': ['ja'],\n",
    "        'files[]': files,\n",
    "        'method': 'tm',\n",
    "        'auto_approve_option': 0,\n",
    "        'import_duplicates': 1,\n",
    "        'apply_untranslated_strings_only': 1,\n",
    "        'perfect_match': 0,\n",
    "        'json': 1\n",
    "    }\n",
    "\n",
    "    return crowdin_request(repository, '/pre-translate', 'POST', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_with_machine(repository, files):\n",
    "    data = {\n",
    "        'languages[]': ['ja'],\n",
    "        'files[]': files,\n",
    "        'method': 'mt',\n",
    "        'engine': 'google',\n",
    "        'json': 1\n",
    "    }\n",
    "\n",
    "    return crowdin_request(repository, '/pre-translate', 'POST', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_translate(repository, translation_needed, file_info):\n",
    "    candidate_files = [get_crowdin_file(repository, file) for file in translation_needed]\n",
    "    translation_files = [file for file in candidate_files if file in file_info]\n",
    "\n",
    "    if len(translation_files) == 0:\n",
    "        return\n",
    "\n",
    "    #translate_with_machine(repository, translation_files)\n",
    "    #translate_with_memory(repository, translation_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_translate_folder(repository, folder, candidate_files, file_info):\n",
    "    prefix = folder + '/'\n",
    "    translation_needed = []\n",
    "\n",
    "    for file in candidate_files:\n",
    "        if file.find(prefix) != 0:\n",
    "            continue\n",
    "\n",
    "        crowdin_file = get_crowdin_file(repository, file)\n",
    "\n",
    "        if crowdin_file not in file_info:\n",
    "            continue\n",
    "\n",
    "        target_file = 'ja/' + file[3:] if file[0:3] == 'en/' else file.replace('/en/', '/ja/')\n",
    "\n",
    "        if not os.path.isfile(target_file):\n",
    "            translation_needed.append(file)\n",
    "\n",
    "    if len(translation_needed) > 0:\n",
    "        logging.info('crowdin-api pre-translate %s' % folder)\n",
    "        pre_translate(repository, translation_needed, file_info)\n",
    "\n",
    "    return translation_needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markdown File Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility methods to convert a folder to a list of files and a list of files into a list of folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(folder):\n",
    "    files = []\n",
    "\n",
    "    for name in os.listdir(folder):\n",
    "        path = '%s/%s' % (folder, name)\n",
    "\n",
    "        if os.path.isdir(path):\n",
    "            files.extend(get_files(path))\n",
    "        else:\n",
    "            files.append(path)\n",
    "\n",
    "    return list(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folders(files):\n",
    "    return sorted(set([os.path.dirname(file) if os.path.isfile(file) else file for file in files]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to worry about translating folders with `.markdown` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_translation_eligible(repository, file, language_id):\n",
    "    prefix = repository.github.project_folder\n",
    "\n",
    "    if file.find(prefix) == 0:\n",
    "        if file[0:3] == language_id + '/' or file.find('/' + language_id + '/') != -1:\n",
    "            if file[-9:] == '.markdown' or file[-3:] == '.md' or file[-5:] == '.html':\n",
    "                return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eligible_files(repository, output, language_id):\n",
    "    if test_folders is not None:\n",
    "        for test_folder in set(test_folders):\n",
    "            test_files.update(get_files(test_folder))\n",
    "\n",
    "        output = '\\n'.join(test_files)\n",
    "\n",
    "    return [\n",
    "        file for file in output.split('\\n')\n",
    "            if is_translation_eligible(repository, file, language_id)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GitHub Issue API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve milestones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_milestone_number(repository, milestone_map, milestone_title):\n",
    "    if milestone_title in milestone_map:\n",
    "        return milestone_map[milestone_title]\n",
    "\n",
    "    api_path = '/repos/%s/milestones' % repository.github.origin\n",
    "\n",
    "    data = {\n",
    "        'title': milestone_title\n",
    "    }\n",
    "\n",
    "    status_code, headers, milestone = filter_github_request(api_path, 'POST', data)\n",
    "\n",
    "    milestone_map[milestone_title] = milestone['number']\n",
    "\n",
    "    return milestone['number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_milestone_map(repository):\n",
    "    milestone_map_folder = '%s/%s' % (initial_dir, repository.github.origin)\n",
    "    \n",
    "    if not os.path.exists(milestone_map_folder):\n",
    "        os.makedirs(milestone_map_folder)\n",
    "    \n",
    "    milestone_map_file = '%s/milestones.json' % milestone_map_folder\n",
    "    milestone_map = {}\n",
    "\n",
    "    if os.path.exists(milestone_map_file):\n",
    "        with open(milestone_map_file, 'r') as f:\n",
    "            milestone_map = json.load(f)\n",
    "    else:\n",
    "        api_path = '/repos/%s/milestones?state=all' % repository.github.origin\n",
    "\n",
    "        status_code, headers, milestones = filter_github_request(api_path)\n",
    "\n",
    "        milestone_map = {\n",
    "            milestone['title']: milestone['number']\n",
    "                for milestone in milestones\n",
    "        }\n",
    "\n",
    "    get_milestone_number(repository, milestone_map, 'human')\n",
    "    get_milestone_number(repository, milestone_map, 'machine')\n",
    "\n",
    "    with open(milestone_map_file, 'w') as f:\n",
    "        json.dump(milestone_map, f)\n",
    "\n",
    "    return milestone_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local cache letting us know how folders are mapped to issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issue_map(repository):\n",
    "    issue_map_file = '%s/%s/issues.json' % (initial_dir, repository.github.origin)\n",
    "    issue_map = {}\n",
    "\n",
    "    if os.path.exists(issue_map_file):\n",
    "        with open(issue_map_file, 'r') as f:\n",
    "            issue_map = json.load(f)\n",
    "\n",
    "    if repository.github.branch not in issue_map:\n",
    "        return {}\n",
    "\n",
    "    prefix = repository.github.project_folder + '/'\n",
    "\n",
    "    return {\n",
    "        key: value for key, value in issue_map[repository.github.branch].items()\n",
    "            if key.find(prefix) == 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_issue_map(repository, updated_issues):\n",
    "    issue_map_file = '%s/%s/issues.json' % (initial_dir, repository.github.origin)\n",
    "    issue_map = {}\n",
    "\n",
    "    if os.path.exists(issue_map_file):\n",
    "        with open(issue_map_file, 'r') as f:\n",
    "            issue_map = json.load(f)\n",
    "\n",
    "    if repository.github.branch not in issue_map:\n",
    "        issue_map[repository.github.branch] = {}\n",
    "\n",
    "    issue_map[repository.github.branch].update(updated_issues)\n",
    "\n",
    "    with open(issue_map_file, 'w') as f:\n",
    "        json.dump(issue_map, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistent way of generating issue bodies from metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issue_line(repository, prefix, key, metadata):\n",
    "    translation_completion = int(100 * metadata['translated'] / metadata['phrases'])\n",
    "    proofread_completion = int(100 * metadata['approved'] / metadata['phrases'])\n",
    "\n",
    "    status_string = '(%d%% translated, %d%% proofread)' % (translation_completion, proofread_completion)\n",
    "\n",
    "    if 'id' not in metadata:\n",
    "        return '\\n**%s** %s\\n' % (key[prefix.rfind('/')+1:], status_string)\n",
    "    else:\n",
    "        return '* [%s](https://crowdin.com/translate/%s/%s/en-ja) %s' % \\\n",
    "            (key[key.rfind('/')+1:], repository.crowdin.project_id, metadata['id'], status_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issue_body(repository, folder, file_info):\n",
    "    prefix = get_crowdin_file(repository, folder)\n",
    "\n",
    "    matching_files = sorted([\n",
    "        (key, metadata) for key, metadata in file_info.items() if key.find(prefix) == 0\n",
    "    ])\n",
    "\n",
    "    if len(matching_files) == 0:\n",
    "        return ''\n",
    "\n",
    "    external_links = [\n",
    "        get_issue_line(repository, prefix, key, metadata)\n",
    "            for key, metadata in matching_files\n",
    "    ]\n",
    "\n",
    "    return '\\n'.join(external_links).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that issues always exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_issue(repository, folder, file_info):\n",
    "    issue_map = get_issue_map(repository)\n",
    "\n",
    "    if folder in issue_map:\n",
    "        return issue_map[folder]\n",
    "\n",
    "    data = {\n",
    "        'title': '%s - %s' % (repository.github.branch, folder),\n",
    "        'body': get_issue_body(repository, folder, file_info),\n",
    "        'milestone': get_milestone_map(repository)['machine']\n",
    "    }\n",
    "\n",
    "    api_path = '/repos/%s/issues' % repository.github.origin\n",
    "    status_code, headers, result = filter_github_request(api_path, 'POST', data)\n",
    "\n",
    "    issue_map[folder] = result['number']\n",
    "\n",
    "    save_issue_map(repository, issue_map)\n",
    "\n",
    "    return issue_map[folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_issues(repository, all_files, file_info):\n",
    "    root_folders = get_root_folders(repository, all_files)\n",
    "\n",
    "    new_translations = []\n",
    "\n",
    "    for folder in root_folders:\n",
    "        issue_number = init_issue(repository, folder, file_info)\n",
    "        new_translations.extend(pre_translate_folder(repository, folder, all_files, file_info))\n",
    "\n",
    "    return new_translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the milestone numbers for all issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_milestone_numbers(repository, all_files, file_info):\n",
    "    root_folders = get_root_folders(repository, all_files)\n",
    "\n",
    "    milestone_numbers = {}\n",
    "\n",
    "    for folder in root_folders:\n",
    "        issue_number = init_issue(repository, folder, file_info)\n",
    "\n",
    "        api_path = '/repos/%s/issues/%d' % (repository.github.origin, issue_number)\n",
    "        status_code, headers, result = filter_github_request(api_path)\n",
    "\n",
    "        state = result['state']\n",
    "\n",
    "        milestone = result['milestone'] if 'milestone' in result else None\n",
    "        milestone_number = milestone['number'] if milestone is not None else None\n",
    "\n",
    "        milestone_numbers[folder] = (state, milestone_number)\n",
    "\n",
    "    return milestone_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reopen the issue on update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reopen_issue(repository, folder, file_info):\n",
    "    issue_number = init_issue(repository, folder, file_info)\n",
    "\n",
    "    api_path = '/repos/%s/issues/%d' % (repository.github.origin, issue_number)\n",
    "    status_code, headers, result = filter_github_request(api_path)\n",
    "\n",
    "    milestone = result['milestone'] if 'milestone' in result else None\n",
    "    milestone_number = milestone['number'] if milestone is not None else None\n",
    "\n",
    "    issue_body = get_issue_body(repository, folder, file_info)\n",
    "\n",
    "    if result['state'] != 'closed' and result['body'] == issue_body and milestone_number is not None:\n",
    "        return issue_number\n",
    "\n",
    "    data = {\n",
    "        'state': 'open',\n",
    "        'body': issue_body\n",
    "    }\n",
    "\n",
    "    if milestone_number is None:\n",
    "        data['milestone'] = get_milestone_map(repository)['machine']\n",
    "\n",
    "    status_code, headers, result = filter_github_request(api_path, 'PATCH', data)\n",
    "\n",
    "    return issue_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reopen_issues(repository, new_files, file_info):\n",
    "    root_folders = get_root_folders(repository, new_files)\n",
    "\n",
    "    new_translations = []\n",
    "\n",
    "    for folder in root_folders:\n",
    "        reopen_issue(repository, folder, file_info)\n",
    "        new_translations.extend(pre_translate_folder(repository, folder, new_files, file_info))\n",
    "\n",
    "    return new_translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close a GitHub issue when translation is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_issue(repository, folder, file_info):\n",
    "    issue_number = init_issue(repository, folder, file_info)\n",
    "\n",
    "    api_path = '/repos/%s/issues/%d' % (repository.github.origin, issue_number)\n",
    "\n",
    "    data = {\n",
    "        'state': 'closed'\n",
    "    }\n",
    "\n",
    "    status_code, headers, result = filter_github_request(api_path, 'PATCH', data)\n",
    "\n",
    "    return issue_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GitHub Project API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the current list of repository projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_project_map(origin):\n",
    "    status_code, headers, projects = filter_github_request('/repos/%s/projects' % origin)\n",
    "\n",
    "    project_map = {}\n",
    "\n",
    "    for project in projects:\n",
    "        project_name = project['name']\n",
    "\n",
    "        separator_pos = project_name.find(' - ')\n",
    "\n",
    "        branch = project_name[:separator_pos]\n",
    "        folder = project_name[separator_pos+3:]\n",
    "\n",
    "        if branch not in project_map:\n",
    "            project_map[branch] = {}\n",
    "\n",
    "        project_map[branch][folder] = project['id']\n",
    "\n",
    "    return project_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an empty project if one does not already exist for the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_github_project(project_map, repository):\n",
    "    if repository.github.branch in project_map:\n",
    "        for project_folder, project_number in project_map[repository.github.branch].items():\n",
    "            if project_folder == repository.github.project_folder:\n",
    "                return project_number\n",
    "    else:\n",
    "        project_map[repository.github.branch] = {}\n",
    "\n",
    "    api_path = '/repos/%s/projects' % repository.github.origin\n",
    "\n",
    "    data = {\n",
    "        'name': '%s - %s' % (repository.github.branch, repository.github.project_folder)\n",
    "    }\n",
    "\n",
    "    status_code, headers, project = filter_github_request(api_path, 'POST', data)\n",
    "    project_map[repository.github.branch][repository.github.project_folder] = project['id']\n",
    "\n",
    "    return project['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the proper columns (\"To do\", \"In progress\", \"Done\") for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_github_column(columns, project_number, column_index, column_name):\n",
    "    for column in columns:\n",
    "        if column_name == column['name']:\n",
    "            return column\n",
    "\n",
    "    api_path = '/projects/%s/columns' % project_number\n",
    "\n",
    "    data = {\n",
    "        'name': column_name\n",
    "    }\n",
    "\n",
    "    status_code, headers, column = filter_github_request(api_path, 'POST', data)\n",
    "\n",
    "    if len(columns) != column_index:\n",
    "        api_path = '/projects/columns/%s/moves' % column['id']\n",
    "\n",
    "        data = {\n",
    "            'position': 'first' if column_index == 0 else 'after:%d' % (column_index - 1)\n",
    "        }\n",
    "\n",
    "    columns.insert(column_index, column)\n",
    "\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_github_columns(column_map, project_number):\n",
    "    if project_number in column_map:\n",
    "        columns = column_map[project_number]\n",
    "    else:\n",
    "        status_code, headers, columns = filter_github_request('/projects/%s/columns' % project_number)\n",
    "        column_map[project_number] = columns\n",
    "\n",
    "    init_github_column(columns, project_number, 0, 'Selected for translation')\n",
    "    init_github_column(columns, project_number, 1, 'Translation started')\n",
    "    init_github_column(columns, project_number, 2, 'Ready for proofreading')\n",
    "    init_github_column(columns, project_number, 3, 'Proofreading started')\n",
    "    init_github_column(columns, project_number, 4, 'Merge request sent')\n",
    "    init_github_column(columns, project_number, 5, 'Merge completed')\n",
    "\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that a card exists for the specified GitHub issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_card(card_map, repository, project_number, issue_number, target_column_index, columns):\n",
    "    if project_number not in card_map:\n",
    "        card_map[project_number] = {}\n",
    "\n",
    "        for column in columns:\n",
    "            column_id = column['id']\n",
    "            column_name = column['name']\n",
    "\n",
    "            status_code, headers, cards = filter_github_request('/projects/columns/%s/cards' % column_id)\n",
    "\n",
    "            card_map[project_number].update({\n",
    "                int(card['content_url'][card['content_url'].rfind('/')+1:]): {\n",
    "                    'card_id': card['id'], 'column_id': column_id\n",
    "                } for card in cards if 'content_url' in card\n",
    "            })\n",
    "\n",
    "    if issue_number in card_map[project_number]:\n",
    "        return card_map[project_number][issue_number]\n",
    "\n",
    "    api_path = '/repos/%s/issues/%d' % (repository.github.origin, issue_number)\n",
    "    status_code, headers, issue = filter_github_request(api_path)\n",
    "\n",
    "    column_index = target_column_index if target_column_index is not None else 0\n",
    "\n",
    "    api_path = '/projects/columns/%s/cards' % columns[column_index]['id']\n",
    "\n",
    "    data = {\n",
    "        'content_id': issue['id'],\n",
    "        'content_type': 'Issue'\n",
    "    }\n",
    "\n",
    "    status_code, headers, card = filter_github_request(api_path, 'POST', data)\n",
    "\n",
    "    card_map[project_number][issue_number] = {\n",
    "        'card_id': card['id'], 'column_id': columns[column_index]['id']\n",
    "    }\n",
    "\n",
    "    return card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the card is in the right column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_issue_column(project_map, column_map, card_map, repository, folder, issue_number, target_column_index):\n",
    "    project_number = init_github_project(project_map, repository)\n",
    "    columns = init_github_columns(column_map, project_number)\n",
    "    target_column_id = None if target_column_index is None else columns[target_column_index]['id']\n",
    "\n",
    "    card = get_github_card(card_map, repository, project_number, issue_number, target_column_index, columns)\n",
    "\n",
    "    if target_column_id is None or card['column_id'] == target_column_id:\n",
    "        return\n",
    "\n",
    "    api_path = '/projects/columns/cards/%s/moves' % card['card_id']\n",
    "\n",
    "    data = {\n",
    "        'position': 'top',\n",
    "        'column_id': target_column_id\n",
    "    }\n",
    "\n",
    "    status_code, headers, update_info = filter_github_request(api_path, 'POST', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_issue_columns(repository, file_info, target_column_map):\n",
    "    project_map = get_github_project_map(repository.github.origin)\n",
    "    column_map = {}\n",
    "    card_map = {}\n",
    "\n",
    "    for folder, target_column_index in target_column_map.items():\n",
    "        init_issue(repository, folder, file_info)\n",
    "        issue_map = get_issue_map(repository)\n",
    "        issue_number = issue_map[folder]\n",
    "\n",
    "        update_issue_column(\n",
    "            project_map, column_map, card_map, repository,\n",
    "            folder, issue_number, target_column_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_translation_issues(repository, file_info):\n",
    "    issues = []\n",
    "\n",
    "    api_path = '/repos/%s/issues?milestone=%d' % \\\n",
    "        (repository.github.origin, get_milestone_map(repository)['human'])\n",
    "\n",
    "    status_code, headers, issues = filter_github_request_all(api_path)\n",
    "\n",
    "    target_column_map = {}\n",
    "\n",
    "    issue_map = get_issue_map(repository)\n",
    "\n",
    "    folder_map = {\n",
    "        value: key for key, value in issue_map.items()\n",
    "    }\n",
    "\n",
    "    for issue in issues:\n",
    "        issue_number = issue['number']\n",
    "\n",
    "        if issue_number not in folder_map:\n",
    "            continue\n",
    "\n",
    "        folder = folder_map[issue_number]\n",
    "        key = get_crowdin_file(repository, folder)\n",
    "\n",
    "        if key in file_info:\n",
    "            approved = file_info[key]['approved']\n",
    "            translated = file_info[key]['translated']\n",
    "            phrases = file_info[key]['phrases']\n",
    "\n",
    "            if approved == phrases:\n",
    "                target_column_map[folder] = 4\n",
    "            elif approved > 0:\n",
    "                target_column_map[folder] = 3\n",
    "            elif translated == phrases:\n",
    "                target_column_map[folder] = 2\n",
    "            elif translated > 0:\n",
    "                target_column_map[folder] = 1\n",
    "            else:\n",
    "                target_column_map[folder] = 0\n",
    "        else:\n",
    "            target_column_map[folder] = None\n",
    "\n",
    "    update_issue_columns(repository, file_info, target_column_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZenDesk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ZenDesk, there are a lot of default parameters we want to pass to the API if none have been specified, such as a consistent sort order, or making sure that we fetch as many results at once. Add a method to let us do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_default_parameter(parameters, name, default_value):\n",
    "    if name not in parameters:\n",
    "        parameters[name] = default_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a method to make requests against the ZenDesk API.\n",
    "\n",
    "Also make sure to provide a workaround for a bug in the ZenDesk incremental API: normally, API expects you to be able to continually use `next_page`. However, if you upload more than 1 page worth of entries within 1 seconds, the `next_page` becomes useless (this can happen if we bulk import articles via API, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zendesk_request(api_path, attribute_name, params=None):\n",
    "    parameters = {}\n",
    "    \n",
    "    if params is not None:\n",
    "        parameters.update(params)\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    set_default_parameter(parameters, 'per_page', 100)    \n",
    "    set_default_parameter(parameters, 'sort_by', 'created_at')\n",
    "    set_default_parameter(parameters, 'page', 1)\n",
    "\n",
    "    api_result = None\n",
    "    page_count = None\n",
    "\n",
    "    incremental = api_path.find('/incremental/') != -1\n",
    "    \n",
    "    while page_count is None or parameters['page'] <= page_count:\n",
    "        query_string = '&'.join('%s=%s' % (key, value) for key, value in parameters.items())\n",
    "        url = 'https://liferay-support.zendesk.com/api/v2%s?%s' % (api_path, query_string)\n",
    "\n",
    "        if url is None:\n",
    "            break\n",
    "\n",
    "        r = session.get(url)\n",
    "        print(url)\n",
    "\n",
    "        api_result = json.loads(r.text)\n",
    "\n",
    "        if attribute_name in api_result:\n",
    "            if type(api_result[attribute_name]) == list:\n",
    "                result = result + api_result[attribute_name]\n",
    "            else:\n",
    "                result.append(api_result[attribute_name])\n",
    "        else:\n",
    "            print(r.text)\n",
    "            return None\n",
    "\n",
    "        parameters['page'] = parameters['page'] + 1\n",
    "\n",
    "        if 'page_count' in api_result:\n",
    "            page_count = api_result['page_count']\n",
    "        elif 'count' in api_result:\n",
    "            page_count = math.ceil(api_result['count'] / parameters['per_page'])\n",
    "        else:\n",
    "            page_count = 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate against ZenDesk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_zendesk():\n",
    "    print('Authenticating with Liferay SAML IdP')\n",
    "    authenticate('https://liferay-support.zendesk.com/access/login', None)\n",
    "    \n",
    "    return zendesk_request('/users/me.json', 'user')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, update all of the ZenDesk articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zendesk_articles():\n",
    "    user = init_zendesk()\n",
    "    print('Authenticated as %s' % user['email'])\n",
    "    assert(user['verified'])\n",
    "\n",
    "    # Reload the articles we already know about\n",
    "    \n",
    "    articles = {}\n",
    "\n",
    "    try:\n",
    "        with open('%s/articles.json' % initial_dir, 'r') as f:\n",
    "            articles = json.load(f)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Fetch new articles with the incremental API\n",
    "    \n",
    "    article_parameters = {\n",
    "        'start_time': 0 if len(articles) == 0 else max([article['updated_at'] for article in articles.values()])\n",
    "    }\n",
    "    \n",
    "    new_articles = zendesk_request('/help_center/incremental/articles.json', 'articles', article_parameters)\n",
    "    \n",
    "    # Override past articles\n",
    "    \n",
    "    articles.update({article['id']: article for article in new_articles})\n",
    "    \n",
    "    # Cache the articles on disk so we can work on them without having to go back to the API\n",
    "    \n",
    "    with open('articles.json', 'w') as f:\n",
    "        json.dump(articles, f)\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def update_zendesk_articles():\n",
    "    user = init_zendesk()\n",
    "    print('Authenticated as %s' % user['email'])\n",
    "    assert(user['verified'])\n",
    "\n",
    "    # Determine the proper folder structure\n",
    "\n",
    "    categories = zendesk_request('/help_center/en-us/categories.json', 'categories')\n",
    "    category_paths = {\n",
    "        category['id']: 'en/' + category['html_url'][category['html_url'].rfind('/'):]\n",
    "            for category in categories\n",
    "    }\n",
    "\n",
    "\n",
    "    sections = zendesk_request('/help_center/en-us/sections.json', 'sections')\n",
    "    section_paths = {\n",
    "        section['id']: category_paths[section['category_id']] + section['html_url'][section['html_url'].rfind('/'):]\n",
    "            for section in sections\n",
    "    }\n",
    "    \n",
    "    articles = get_zendesk_articles()\n",
    "\n",
    "    article_paths = {\n",
    "        str(article['id']): section_paths[article['section_id']] + article['html_url'][article['html_url'].rfind('/'):] + '.html'\n",
    "            for article in articles.values()\n",
    "                if article['section_id'] in section_paths and not article['draft'] and article['locale'] == 'en-us' and 'Fast Track' not in article['label_names']\n",
    "    }\n",
    "\n",
    "    article_paths.update({\n",
    "        str(article['id']): 'en/' + ('0'*12) + '-Fast-Track' + article['html_url'][article['html_url'].rfind('/'):] + '.html'\n",
    "            for article in articles.values()\n",
    "                if article['section_id'] in section_paths and not article['draft'] and article['locale'] == 'en-us' and 'Fast Track' in article['label_names']\n",
    "    })\n",
    "\n",
    "    for article_id, article_path in article_paths.items():\n",
    "        article_file_name = '/home/minhchau/Work/liferay/zendesk-articles/%s' % article_path\n",
    "        article_folder = os.path.dirname(article_file_name)\n",
    "\n",
    "        if not os.path.exists(article_folder):\n",
    "            os.makedirs(article_folder)\n",
    "\n",
    "        with open(article_file_name, 'w', encoding='utf-8') as f:\n",
    "            f.write(articles[article_id]['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cron job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the list of all files we should translate and all updated files that we need to re-translated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_branch_files(repository):\n",
    "    if repository.github.upstream is not None:\n",
    "        _git('fetch', 'upstream', '--no-tags', '%s:refs/remotes/upstream/%s' % \\\n",
    "             (repository.github.branch, repository.github.branch))\n",
    "\n",
    "        diff_output = _git('diff', '--name-only', '%s..upstream/%s' % \\\n",
    "            (repository.github.branch, repository.github.branch))\n",
    "    else:\n",
    "        diff_output = '\\n'.join([line[3:] for line in _git('status', '-s') if len(line) > 3])\n",
    "\n",
    "    new_files = get_eligible_files(repository, diff_output, 'en')\n",
    "\n",
    "    lstree_output = _git('ls-tree', '-r', '--name-only', repository.github.branch)\n",
    "    all_files = get_eligible_files(repository, lstree_output, 'en')\n",
    "\n",
    "    return new_files, all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep CrowdIn and GitHub translations in sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sources(repository, new_files):\n",
    "    _git('checkout', repository.github.branch)\n",
    "    \n",
    "    if repository.github.upstream:\n",
    "        _git('merge', 'upstream/%s' % repository.github.branch)\n",
    "\n",
    "    return crowdin_upload_sources(repository, new_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_translations(repository, all_files):\n",
    "    now = datetime.now()\n",
    "\n",
    "    status_output = '\\n'.join([line[3:] for line in _git('status', '--porcelain').split('\\n')])\n",
    "    commit_files = get_eligible_files(repository, status_output, 'ja')\n",
    "\n",
    "    if len(commit_files) == 0:\n",
    "        return\n",
    "\n",
    "    for file in commit_files:\n",
    "        if file[-3:] == '.md':\n",
    "            continue\n",
    "\n",
    "        md_file = file[:-9] + '.md'\n",
    "\n",
    "        if os.path.isfile(md_file):\n",
    "            os.remove(md_file)\n",
    "            os.rename(file, md_file)\n",
    "\n",
    "    status_output = '\\n'.join([line[3:] for line in _git('status', '--porcelain').split('\\n')])\n",
    "    commit_files = get_eligible_files(repository, status_output, 'ja')\n",
    "\n",
    "    if len(commit_files) == 0:\n",
    "        return\n",
    "\n",
    "    _git('add', *commit_files)\n",
    "    _git('commit', '-m', 'Updated translations %s' % now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform any needed GitHub issue maintenance to keep project boards usable, perform any needed CrowdIn file maintenance to stay under the quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_files(repository, all_files, old_file_info, new_file_info):\n",
    "    human_milestone_number = get_milestone_map(repository)['human']\n",
    "    milestone_numbers = get_milestone_numbers(repository, all_files, new_file_info)\n",
    "\n",
    "    new_keys = set(new_file_info.keys()).difference(set(old_file_info.keys()))\n",
    "\n",
    "    to_delete = set()\n",
    "    to_upload = set()\n",
    "\n",
    "    for key, metadata in new_file_info.items():\n",
    "        if key not in new_keys or 'id' not in metadata:\n",
    "            continue\n",
    "\n",
    "        if key.find(repository.crowdin.dest_folder) != 0:\n",
    "            continue\n",
    "\n",
    "        local_file = get_local_file(repository, key)\n",
    "        folder = get_root_folders(repository, [local_file])[0]\n",
    "\n",
    "        if folder not in milestone_numbers:\n",
    "            continue\n",
    "\n",
    "        state, milestone_number = milestone_numbers[folder]\n",
    "\n",
    "        to_delete.add(key)\n",
    "\n",
    "        if milestone_number == human_milestone_number:\n",
    "            to_upload.add(local_file)\n",
    "\n",
    "    for key in to_delete:\n",
    "        delete_translation(repository, key)\n",
    "        del new_file_info[key]\n",
    "\n",
    "    return update_sources(repository, to_upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update local copies of translations, translation memory, and glossaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_repository(repository):\n",
    "    logging.info('cd %s' % repository.github.git_root)\n",
    "    os.chdir(repository.github.git_root)\n",
    "\n",
    "    new_files, all_files = get_branch_files(repository)\n",
    "\n",
    "    for file in new_files:\n",
    "        target_file = 'ja/' + file[3:] if file[0:3] == 'en/' else file.replace('/en/', '/ja/')\n",
    "\n",
    "        if os.path.isfile(target_file):\n",
    "            os.remove(target_file)\n",
    "\n",
    "    for file in set(all_files).difference(set(new_files)):\n",
    "        target_file = 'ja/' + file[3:] if file[0:3] == 'en/' else file.replace('/en/', '/ja/')\n",
    "\n",
    "        if not os.path.isfile(target_file):\n",
    "            new_files.append(file)\n",
    "    \n",
    "    old_file_info, file_info = update_sources(repository, new_files)\n",
    "\n",
    "    new_translations = init_issues(repository, all_files, file_info)\n",
    "    new_translation_needed = list(set(new_files).difference(set(new_translations)))\n",
    "\n",
    "    if len(get_root_folders(repository, new_translation_needed)) > 0:\n",
    "        _, file_info = update_sources(repository, new_translation_needed)\n",
    "        reopen_issues(repository, new_translation_needed, file_info)\n",
    "        new_translations.extend(new_translation_needed)\n",
    "\n",
    "    crowdin_download_translations(repository, all_files, new_files, file_info)\n",
    "\n",
    "    update_translations(repository, all_files)\n",
    "    cleanup_files(repository, all_files, old_file_info, file_info)\n",
    "    update_translation_issues(repository, file_info)\n",
    "\n",
    "    save_translation_memory(repository)\n",
    "    save_glossary(repository)\n",
    "\n",
    "    logging.info('cd -')\n",
    "    os.chdir(initial_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(initial_dir)\n",
    "\n",
    "update_zendesk_articles()\n",
    "\n",
    "for repository in repositories:\n",
    "    update_repository(repository)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Notebook to Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will use `jupyter nbconvert` to build an `api_testing.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "var notebook_name = window.document.getElementById('notebook_name').innerHTML;\n",
    "var script_file = notebook_name + '.py';\n",
    "var nbconvert_command = 'jupyter nbconvert --stdout --to script ' + notebook_name;\n",
    "\n",
    "var grep_command = \"grep -v '^#' | grep -v -F get_ipython | sed '/^$/N;/^\\\\n$/D'\";\n",
    "var command = '!' + nbconvert_command + ' | ' + grep_command + ' > ' + script_file;\n",
    "\n",
    "if (Jupyter.notebook.kernel) {\n",
    "    Jupyter.notebook.kernel.execute(command);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

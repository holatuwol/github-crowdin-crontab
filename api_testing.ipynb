{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "try:\n",
    "    from subprocess import DEVNULL\n",
    "except ImportError:\n",
    "    import os\n",
    "    DEVNULL = open(os.devnull, 'wb')\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `test_folders` to `None` if no longer testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folders = [\n",
    "    'en/develop/tutorials/articles/01-introduction-to-liferay-development'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Git CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _git(*args, stderr=PIPE):\n",
    "    cmd = ['git'] + list(args)\n",
    "    \n",
    "    if args[0] != 'config':\n",
    "        logging.info(' '.join(cmd))\n",
    "    \n",
    "    pipe = Popen(cmd, stdout=PIPE, stderr=stderr)\n",
    "    out, err = pipe.communicate()\n",
    "\n",
    "    return out.decode('UTF-8', 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GitHubRepository = namedtuple(\n",
    "    'GitHubRepository',\n",
    "    ' '.join(['git_root', 'origin', 'upstream', 'branch', 'project_folder'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrowdInRepository = namedtuple(\n",
    "    'CrowdinRepository',\n",
    "    ' '.join(['project_id', 'api_key'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TranslationRepository = namedtuple(\n",
    "    'TranslationRepository',\n",
    "    ' '.join(['github', 'crowdin'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repository(git_root, branch, project_folder, project_id):\n",
    "    os.chdir(git_root)\n",
    "\n",
    "    origin_url = _git('remote', 'get-url', 'origin').strip()\n",
    "    origin = origin_url.split(':')[1][:-4]\n",
    "\n",
    "    upstream_url = _git('remote', 'get-url', 'upstream').strip()\n",
    "    upstream = upstream_url.split(':')[1][:-4]\n",
    "    \n",
    "    api_key = _git('config', 'crowdin.api-key.%s' % project_id).strip()\n",
    "    \n",
    "    os.chdir(initial_dir)\n",
    "    \n",
    "    return TranslationRepository(\n",
    "        GitHubRepository(git_root, origin, upstream, branch, project_folder),\n",
    "        CrowdInRepository(project_id, api_key)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repositories = []\n",
    "\n",
    "for branch in ['7.1.x']:\n",
    "    repositories.append(\n",
    "        get_repository(\n",
    "            '/home/minhchau/Work/liferay/liferay-docs', branch,\n",
    "            'en/develop/tutorials/articles',\n",
    "            'liferay-documentation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GitHub API Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an OAuth token, a base URL, and an API path, we make requests against GitHub by using the `requests` module, and we interpret the response as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_oauth_token = _git('config', 'github.oauth-token').strip()\n",
    "assert(github_oauth_token is not None)\n",
    "\n",
    "github_base_url = 'https://api.github.com'\n",
    "\n",
    "def github_request(api_path, request_type=None, data=None):\n",
    "    headers = {\n",
    "        'user-agent': 'python',\n",
    "        'authorization': 'token %s' % github_oauth_token,\n",
    "        'accept': 'application/vnd.github.inertia-preview+json'\n",
    "    }\n",
    "    \n",
    "    if data is None:\n",
    "        r = requests.get(github_base_url + api_path, headers=headers)\n",
    "    elif request_type == 'PATCH':\n",
    "        r = requests.patch(github_base_url + api_path, json=data, headers=headers)\n",
    "    elif request_type == 'POST':\n",
    "        r = requests.post(github_base_url + api_path, json=data, headers=headers)\n",
    "\n",
    "    if r.status_code < 200 or r.status_code >= 400:\n",
    "        logging.error('HTTP Error: %d' % r.status_code)\n",
    "        return (r.status_code, r.headers, None)\n",
    "\n",
    "    return (r.status_code, r.headers, r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the general privacy restrictions, the GitHub API also adds a rate limit to the REST API endpoints. We'll want to make sure that any request we send to GitHub is aware of those limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global remaining\n",
    "\n",
    "remaining = 0\n",
    "\n",
    "def wait_for_rate_limit_reset():\n",
    "    global remaining\n",
    "\n",
    "    if remaining > 0:\n",
    "        remaining -= 1\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        status_code, headers, result = github_request('/rate_limit')\n",
    "        \n",
    "        resources = result['resources']['core']\n",
    "        remaining = resources['remaining']\n",
    "\n",
    "        if remaining > 0:\n",
    "            remaining -= 1\n",
    "            return\n",
    "\n",
    "        wait_time = 1 + int(resources['reset'] - datetime.now().timestamp())\n",
    "\n",
    "        logging.error('Waiting %d seconds for rate limit reset' % wait_time)\n",
    "\n",
    "        time.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to waiting for the rate limit to reset, we also need to think about what happens if we make multiple requests against the API.\n",
    "\n",
    "For example, let's say we had an hourly script that routinely crawled all the forks of the `liferay-portal` repository checking for new pull requests or new pull request comments. When we do this, we'll also want to make sure that when we make a request to GitHub, that we exclude data that overlaps with data we've already retrieved. This filtering method both waits for the rate limit, and then filters the results, after making the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_github_request(api_path, request_type=None, data=None, field_name=None, min_field_value=None):\n",
    "    global remaining\n",
    "\n",
    "    if remaining == 0:\n",
    "        wait_for_rate_limit_reset()\n",
    "\n",
    "    status_code, headers, result = github_request(api_path, request_type, data)\n",
    "    \n",
    "    if result is None:\n",
    "        remaining = int(headers['X-RateLimit-Remaining']) if 'X-RateLimit-Remaining' in headers else 0\n",
    "\n",
    "        if remaining == 0:\n",
    "            wait_for_rate_limit_reset()\n",
    "            status_code, headers, result = github_request(api_path, request_type, data)\n",
    "\n",
    "    if result is None:\n",
    "        return (status_code, headers, [])\n",
    "\n",
    "    if min_field_value is None:\n",
    "        return (status_code, headers, result)\n",
    "\n",
    "    return (status_code, headers, [item for item in result if item[field_name] > min_field_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_github_request_all(api_path, request_type=None, data=None, field_name=None, min_field_value=None):\n",
    "    results = []\n",
    "    \n",
    "    while api_path is not None:\n",
    "        status_code, headers, new_results = filter_github_request(\n",
    "            api_path, request_type, data, field_name, min_field_value)\n",
    "\n",
    "        results.extend(new_results)\n",
    "        \n",
    "        lower_headers = {\n",
    "            key.lower(): value for key, value in headers.items()\n",
    "        }\n",
    "\n",
    "        if 'link' not in lower_headers:\n",
    "            break\n",
    "        \n",
    "        api_path = None\n",
    "\n",
    "        for link in lower_headers['link'].split(','):\n",
    "            url_info, rel_info = [info.strip() for info in link.split(';')]\n",
    "\n",
    "            rel = rel_info.split('\"')[1]\n",
    "            print(rel)\n",
    "            if rel == 'next':\n",
    "                api_path = url_info[len('<https://api.github.com'):-1]\n",
    "\n",
    "    return status_code, headers, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll want to make sure that our OAuth token actually works and can visit the GitHub repositories we've defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_repository_accessible(reviewer_url):\n",
    "    logging.info('github check-repo-access %s' % reviewer_url)\n",
    "\n",
    "    api_path = '/repos/%s' % reviewer_url\n",
    "\n",
    "    status_code, headers, result = filter_github_request(api_path)\n",
    "    \n",
    "    return result is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for git_repository, crowdin_repository in repositories:\n",
    "    assert(is_repository_accessible(git_repository.origin))\n",
    "    assert(is_repository_accessible(git_repository.upstream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrowdIn CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crowdin(*args, stderr=PIPE):\n",
    "    cmd = ['crowdin'] + list(args)\n",
    "    \n",
    "    logging.info(' '.join(cmd))\n",
    "    \n",
    "    pipe = Popen(cmd, stdout=PIPE, stderr=stderr)\n",
    "    out, err = pipe.communicate()\n",
    "\n",
    "    return out.decode('UTF-8', 'replace').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pandoc` to disable word wrapping to improve machine translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pandoc(filename, *args):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        title_pos = -1\n",
    "        toc_pos = -1\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            if title_pos == -1 and line.find('#') == 0:\n",
    "                title_pos = i\n",
    "            elif line.find('[TOC') == 0:\n",
    "                toc_pos = i\n",
    "        \n",
    "        head_lines = ''.join(lines[0:max(title_pos, toc_pos)+1])\n",
    "        tail_lines = ''.join(lines[max(title_pos, toc_pos)+1:])\n",
    "\n",
    "    cmd = ['pandoc'] + list(args)\n",
    "\n",
    "    pipe = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
    "    out, err = pipe.communicate(input=tail_lines.encode('UTF-8'))\n",
    "    \n",
    "    nowrap_lines = out.decode('UTF-8', 'replace')\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(head_lines)\n",
    "        f.write('\\n')\n",
    "        f.write(nowrap_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a `crowdin.yaml` file to tell the CLI what to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_crowdin(repository, files):\n",
    "    files_json = json.dumps([{\n",
    "        'source': file + '/*.markdown' if os.path.isdir(file) else file,\n",
    "        'translation': '%two_letters_code%/' + file[3:] + '/%original_file_name%'\n",
    "    } for file in files], indent=2)\n",
    "\n",
    "    with open('%s/crowdin.yaml' % repository.github.git_root, 'w') as f:\n",
    "        f.write('''\n",
    "\"project_identifier\" : \"{crowdin_project_id}\"\n",
    "\"api_key\" : \"{crowdin_api_key}\"\n",
    "\"base_path\" : \"{git_root}\"\n",
    "\"preserve_hierarchy\": true\n",
    "\n",
    "\"files\": {files}\n",
    "'''.format(\n",
    "        crowdin_project_id=repository.crowdin.project_id,\n",
    "        crowdin_api_key=repository.crowdin.api_key,\n",
    "        git_root=repository.github.git_root,\n",
    "        files=files_json\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper functions to upload sources and download translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowdin_upload_sources(repository, new_files):\n",
    "    for file in new_files:\n",
    "        _pandoc(file, '--from=gfm', '--to=gfm', '--wrap=none')\n",
    "    \n",
    "    if len(new_files) > 0:\n",
    "        configure_crowdin(repository, new_files)\n",
    "        _crowdin('upload', 'sources', '-b', repository.github.branch)\n",
    "\n",
    "    _git('reset', '--hard')\n",
    "        \n",
    "    return get_crowdin_file_info(repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowdin_download_translations(repository, all_files):\n",
    "    configure_crowdin(repository, get_folders(all_files))\n",
    "    \n",
    "    _crowdin('download', '-l', 'ja', '-b', repository.github.branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrowdIn API Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crowdin_base_url = 'https://api.crowdin.com/api/project/'\n",
    "\n",
    "def crowdin_request(repository, api_path, request_type='GET', data=None, files=None):\n",
    "    headers = {\n",
    "        'user-agent': 'python'\n",
    "    }\n",
    "    \n",
    "    request_url = crowdin_base_url + repository.crowdin.project_id + api_path + \\\n",
    "        '?key=' + repository.crowdin.api_key\n",
    "    \n",
    "    if request_type == 'GET':\n",
    "        r = requests.get(request_url, data=data, headers=headers)\n",
    "    else:\n",
    "        r = requests.post(request_url, data=data, files=files, headers=headers)\n",
    "\n",
    "    if r.status_code < 200 or r.status_code >= 400:\n",
    "        logging.error('HTTP Error: %d' % r.status_code)\n",
    "        return (r.status_code, None)\n",
    "\n",
    "    return (r.status_code, r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crowdin File Management API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API calls to download the translation memory and glossary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_translation_memory(repository):\n",
    "    logging.info('crowdin-api download-tm')\n",
    "\n",
    "    data = {\n",
    "        'source_language': 'en',\n",
    "        'target_language': 'ja'\n",
    "    }\n",
    "    \n",
    "    status_code, response_text = crowdin_request(repository, '/download-tm', 'GET', data)\n",
    "\n",
    "    if response_text is not None:\n",
    "        with open('%s/%s.tmx' % (initial_dir, repository.crowdin.project_id), 'w') as f:\n",
    "            f.write(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_glossary(repository):\n",
    "    logging.info('crowdin-api download-glossary')\n",
    "\n",
    "    status_code, response_text = crowdin_request(repository, '/download-glossary', 'GET')\n",
    "\n",
    "    if response_text is not None:\n",
    "        with open('%s/%s.tbx' % (initial_dir, repository.crowdin.project_id), 'w') as f:\n",
    "            f.write(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API call to delete existing translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_translation(repository, file):\n",
    "    logging.info('crowdin-api delete-file %s' % file)\n",
    "    \n",
    "    data = {\n",
    "        'file': file[len(repository.github.branch)+1:],\n",
    "        'branch': repository.github.branch\n",
    "    }\n",
    "    \n",
    "    return crowdin_request(repository, '/delete-file', 'POST', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API calls to find the file IDs for CrowdIn (used to generate links within the issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_crowdin_file_info(files_element, current_path, file_info):\n",
    "    for item in files_element.children:\n",
    "        if item.name != 'item':\n",
    "            continue\n",
    "        \n",
    "        item_name = item.find('name').text\n",
    "        item_node_type = item.find('node_type').text\n",
    "\n",
    "        item_path = current_path + '/' + item_name if current_path is not None else item_name\n",
    "\n",
    "        file_info[item_path] = {\n",
    "            'phrases': int(item.find('phrases').text),\n",
    "            'approved': int(item.find('approved').text)\n",
    "        }\n",
    "        \n",
    "        if item_node_type == 'file':\n",
    "            file_info[item_path]['id'] = item.find('id').text\n",
    "        else:\n",
    "            extract_crowdin_file_info(item.find('files'), item_path, file_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crowdin_file_info(repository):\n",
    "    logging.info('crowdin-api language-status')\n",
    "\n",
    "    data = {\n",
    "        'language': 'ja'\n",
    "    }\n",
    "    \n",
    "    status_code, response_text = crowdin_request(\n",
    "        repository, '/language-status', 'POST', data)\n",
    "    \n",
    "    file_info = {}\n",
    "\n",
    "    if response_text is not None:\n",
    "        soup = BeautifulSoup(response_text)\n",
    "        extract_crowdin_file_info(soup.find('files'), None, file_info)\n",
    "\n",
    "    return file_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crowdin Translation API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send requests to CrowdIn to do automated translation (translation memory, then machine translation, then translation memory again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_with_memory(repository, files):\n",
    "    logging.info('crowdin-api pre-translate tm -b %s' % repository.github.branch)\n",
    "\n",
    "    data = {\n",
    "        'languages[]': ['ja'],\n",
    "        'files[]': files,\n",
    "        'method': 'tm',\n",
    "        'auto_approve_option': 0,\n",
    "        'import_duplicates': 1,\n",
    "        'apply_untranslated_strings_only': 1,\n",
    "        'perfect_match': 0,\n",
    "        'json': 1\n",
    "    }\n",
    "    \n",
    "    return crowdin_request(repository, '/pre-translate', 'POST', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_with_machine(repository, files):\n",
    "    logging.info('crowdin-api pre-translate mt -b %s' % repository.github.branch)\n",
    "\n",
    "    data = {\n",
    "        'languages[]': ['ja'],\n",
    "        'files[]': files,\n",
    "        'method': 'mt',\n",
    "        'engine': 'google',\n",
    "        'json': 1\n",
    "    }\n",
    "\n",
    "    return crowdin_request(repository, '/pre-translate', 'POST', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_translate(repository, translation_needed, file_info):\n",
    "    translation_files = set()\n",
    "    \n",
    "    for file in translation_needed:\n",
    "        prefix = '%s/%s' % (repository.github.branch, file)\n",
    "        translation_files.update([key for key in file_info.keys() if key.find(prefix) == 0])\n",
    "    \n",
    "    if len(translation_files) == 0:\n",
    "        return\n",
    "\n",
    "    translate_with_memory(repository, translation_files)\n",
    "    translate_with_machine(repository, translation_files)\n",
    "    translate_with_memory(repository, translation_files)\n",
    "\n",
    "    crowdin_download_translations(repository, translation_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markdown File Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility methods to convert a folder to a list of files and a list of files into a list of folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(folder):\n",
    "    files = []\n",
    "    \n",
    "    for name in os.listdir(folder):\n",
    "        path = '%s/%s' % (folder, name)\n",
    "\n",
    "        if os.path.isdir(path):\n",
    "            files.extend(get_files(path))\n",
    "        else:\n",
    "            files.append(path)\n",
    "    \n",
    "    return list(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folders(files):\n",
    "    return sorted(set([os.path.dirname(file) if os.path.isfile(file) else file for file in files]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to worry about translating folders with `.markdown` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_translation_eligible(file, language_id):\n",
    "    if file[0:2] != language_id or file[2] != '/':\n",
    "        return False\n",
    "\n",
    "    if file[-9:] == '.markdown' or file[-3:] == '.md':\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_md_files(output, language_id):\n",
    "    if test_folders is not None:\n",
    "        test_files = set()\n",
    "        \n",
    "        for test_folder in set(test_folders):\n",
    "            test_files.update(get_files(test_folder))\n",
    "    \n",
    "        return sorted([file for file in test_files if is_translation_eligible(file, language_id)])\n",
    "    \n",
    "    files = output.split('\\n')\n",
    "    \n",
    "    get_folders()\n",
    "\n",
    "    return [file for file in files if is_translation_eligible(file, language_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GitHub Issue API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve milestones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_milestone_number(repository, milestone_map, milestone_title):\n",
    "    if milestone_title in milestone_map:\n",
    "        return milestone_map[milestone_title]\n",
    "    \n",
    "    api_path = '/repos/%s/milestones' % repository.github.origin\n",
    "\n",
    "    data = {\n",
    "        'title': milestone_title\n",
    "    }\n",
    "    \n",
    "    status_code, headers, milestone = filter_github_request(api_path, 'POST', data)\n",
    "    \n",
    "    milestone_map[milestone_title] = milestone['number']\n",
    "    \n",
    "    return milestone['number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_milestone_map(repository):\n",
    "    milestone_map_file = '%s/%s/milestones.json' % (initial_dir, repository.github.origin)\n",
    "    milestone_map = {}\n",
    "\n",
    "    if os.path.exists(milestone_map_file):\n",
    "        with open(milestone_map_file, 'r') as f:\n",
    "            milestone_map = json.load(f)\n",
    "    else:\n",
    "        api_path = '/repos/%s/milestones?state=all' % repository.github.origin\n",
    "    \n",
    "        status_code, headers, milestones = filter_github_request(api_path)\n",
    "        \n",
    "        milestone_map = {\n",
    "            milestone['title']: milestone['number']\n",
    "                for milestone in milestones\n",
    "        }\n",
    "        \n",
    "    get_milestone_number(repository, milestone_map, 'human')\n",
    "    get_milestone_number(repository, milestone_map, 'machine')\n",
    "\n",
    "    with open(milestone_map_file, 'w') as f:\n",
    "        json.dump(milestone_map, f)\n",
    "\n",
    "    return milestone_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local cache letting us know how folders are mapped to issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issue_map(repository):\n",
    "    issue_map_file = '%s/%s/issues.json' % (initial_dir, repository.github.origin)\n",
    "    issue_map = {}\n",
    "\n",
    "    if os.path.exists(issue_map_file):\n",
    "        with open(issue_map_file, 'r') as f:\n",
    "            issue_map = json.load(f)\n",
    "\n",
    "    return issue_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_issue_map(repository, issue_map):\n",
    "    issue_map_file = '%s/%s/issues.json' % (initial_dir, repository.github.origin)\n",
    "\n",
    "    with open(issue_map_file, 'w') as f:\n",
    "        json.dump(issue_map, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistent way of generating issue bodies from metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issue_line(repository, prefix, key, metadata):\n",
    "    translation_completion = int(100 * metadata['approved'] / metadata['phrases'])\n",
    "    \n",
    "    if 'id' not in metadata:\n",
    "        return '\\n**%s (%s%%)**\\n' % \\\n",
    "            (key[prefix.rfind('/')+1:], translation_completion)\n",
    "    else:\n",
    "        return '* [%s](https://crowdin.com/translate/%s/%s/en-ja) (%s%%)' % \\\n",
    "            (key[key.rfind('/')+1:], repository.crowdin.project_id, metadata['id'], translation_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issue_body(repository, folder, file_info):\n",
    "    prefix = '%s/%s' % (repository.github.branch, folder)\n",
    "\n",
    "    matching_files = sorted([\n",
    "        (key, metadata) for key, metadata in file_info.items() if key.find(prefix) == 0\n",
    "    ])\n",
    "    \n",
    "    if len(matching_files) == 0:\n",
    "        return ''\n",
    "\n",
    "    external_links = [\n",
    "        get_issue_line(repository, prefix, key, metadata)\n",
    "            for key, metadata in matching_files\n",
    "    ]\n",
    "    \n",
    "    return '\\n'.join(external_links).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that issues always exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_issue(repository, folder, file_info):\n",
    "    issue_map = get_issue_map(repository)\n",
    "    \n",
    "    if repository.github.branch not in issue_map:\n",
    "        issue_map[repository.github.branch] = {}\n",
    "    \n",
    "    issues = issue_map[repository.github.branch]\n",
    "    \n",
    "    if folder in issues:\n",
    "        return False, issues[folder]\n",
    "\n",
    "    logging.info('github create-issue')\n",
    "    \n",
    "    data = {\n",
    "        'title': '%s - %s' % (repository.github.branch, folder),\n",
    "        'body': get_issue_body(repository, folder, file_info),\n",
    "        'milestone': get_milestone_map(repository)['machine']\n",
    "    }\n",
    "    \n",
    "    api_path = '/repos/%s/issues' % repository.github.origin\n",
    "    status_code, headers, result = filter_github_request(api_path, 'POST', data)\n",
    "    \n",
    "    issues[folder] = result['number']\n",
    "\n",
    "    save_issue_map(repository, issue_map)\n",
    "\n",
    "    return True, issues[folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_issues(repository, all_files, file_info):\n",
    "    all_folders = get_folders(all_files)\n",
    "    new_translation_needed = []\n",
    "\n",
    "    for folder in all_folders:\n",
    "        new_issue, issue_number = init_issue(repository, folder, file_info)\n",
    "        \n",
    "        if new_issue:\n",
    "            new_translation_needed.append(folder)\n",
    "\n",
    "    return new_translation_needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reopen the issue on update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reopen_issue(repository, folder, file_info):\n",
    "    new_issue, issue_number = init_issue(repository, folder, file_info)\n",
    "\n",
    "    api_path = '/repos/%s/issues/%d' % (repository.github.origin, issue_number)\n",
    "    status_code, headers, result = filter_github_request(api_path)\n",
    "    \n",
    "    milestone = result['milestone'] if 'milestone' in result else None\n",
    "    milestone_number = milestone['number'] if milestone is not None else None\n",
    "\n",
    "    issue_body = get_issue_body(repository, folder, file_info)\n",
    "\n",
    "    if result['state'] != 'closed' and result['body'] == issue_body and milestone_number is not None:\n",
    "        return new_issue, issue_number\n",
    "\n",
    "    data = {\n",
    "        'state': 'open',\n",
    "        'body': issue_body\n",
    "    }\n",
    "    \n",
    "    if milestone_number is None:\n",
    "        data['milestone'] = get_milestone_map(repository)['machine']\n",
    "\n",
    "    status_code, headers, result = filter_github_request(api_path, 'PATCH', data)\n",
    "    \n",
    "    return new_issue, issue_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reopen_issues(repository, new_files, file_info):\n",
    "    new_folders = get_folders(new_files)\n",
    "    new_translation_needed = list(new_files)\n",
    "    \n",
    "    for folder in new_folders:\n",
    "        new_issue, issue_number = reopen_issue(repository, folder, file_info)\n",
    "        \n",
    "        if new_issue:\n",
    "            new_translation_needed = [\n",
    "                file for file in new_translation_needed\n",
    "                    if file != folder and file.find(folder + '/') != 0\n",
    "            ]\n",
    "\n",
    "            new_translation_needed.append(folder)\n",
    "    \n",
    "    return new_translation_needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close a GitHub issue when translation is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_issue(repository, folder, file_info):\n",
    "    new_issue, issue_number = init_issue(repository, folder, file_info)\n",
    "\n",
    "    api_path = '/repos/%s/issues/%d' % (repository.github.origin, issue_number)\n",
    "\n",
    "    data = {\n",
    "        'state': 'closed'\n",
    "    }\n",
    "\n",
    "    status_code, headers, result = filter_github_request(api_path, 'PATCH', data)\n",
    "    \n",
    "    return new_issue, issue_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GitHub Project API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the current list of repository projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_project_map(origin):\n",
    "    status_code, headers, projects = filter_github_request('/repos/%s/projects' % origin)\n",
    "    \n",
    "    project_map = {}\n",
    "    \n",
    "    for project in projects:\n",
    "        project_name = project['name']\n",
    "        \n",
    "        separator_pos = project_name.find(' - ')\n",
    "        \n",
    "        branch = project_name[:separator_pos]\n",
    "        folder = project_name[separator_pos+3:]\n",
    "        \n",
    "        if branch not in project_map:\n",
    "            project_map[branch] = {}\n",
    "        \n",
    "        project_map[branch][folder] = project['id']\n",
    "    \n",
    "    return project_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an empty project if one does not already exist for the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_github_project(project_map, repository):\n",
    "    for project_folder, project_number in project_map[repository.github.branch].items():\n",
    "        if project_folder == repository.github.project_folder:\n",
    "            return project_number\n",
    "\n",
    "    api_path = '/repos/%s/projects' % repository.github.origin\n",
    "\n",
    "    data = {\n",
    "        'name': '%s - %s' % (repository.github.branch, repository.github.project_folder)\n",
    "    }\n",
    "\n",
    "    status_code, headers, project = filter_github_request(api_path, 'POST', data)\n",
    "    project_map[repository.github.branch][repository.github.project_folder] = project['id']\n",
    "    \n",
    "    return project['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the proper columns (\"To do\", \"In progress\", \"Done\") for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_github_column(columns, project_number, column_index, column_name):\n",
    "    for column in columns:\n",
    "        if column_name == column['name']:\n",
    "            return column\n",
    "    \n",
    "    api_path = '/projects/%s/columns' % project_number\n",
    "    \n",
    "    data = {\n",
    "        'name': column_name\n",
    "    }\n",
    "\n",
    "    status_code, headers, column = filter_github_request(api_path, 'POST', data)\n",
    "\n",
    "    if len(columns) != column_index:\n",
    "        api_path = '/projects/columns/%s/moves' % column['id']\n",
    "        \n",
    "        data = {\n",
    "            'position': 'first' if column_index == 0 else 'after:%d' % (column_index - 1)\n",
    "        }\n",
    "    \n",
    "    columns.insert(column_index, column)\n",
    "    \n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_github_columns(column_map, project_number):\n",
    "    if project_number in column_map:\n",
    "        columns = column_map[project_number]\n",
    "    else:\n",
    "        status_code, headers, columns = filter_github_request('/projects/%s/columns' % project_number)\n",
    "        column_map[project_number] = columns\n",
    "    \n",
    "    init_github_column(columns, project_number, 0, 'To do')\n",
    "    init_github_column(columns, project_number, 1, 'In progress')\n",
    "    init_github_column(columns, project_number, 2, 'Done')\n",
    "    \n",
    "    return columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that a card exists for the specified GitHub issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_card(card_map, repository, project_number, issue_number, target_column_index, columns):\n",
    "    if project_number not in card_map:\n",
    "        card_map[project_number] = {}\n",
    "        \n",
    "        for column in columns:\n",
    "            column_id = column['id']\n",
    "            column_name = column['name']\n",
    "\n",
    "            status_code, headers, cards = filter_github_request('/projects/columns/%s/cards' % column_id)\n",
    "\n",
    "            card_map[project_number].update({\n",
    "                int(card['content_url'][card['content_url'].rfind('/')+1:]): {\n",
    "                    'card_id': card['id'], 'column_id': column_id\n",
    "                } for card in cards if 'content_url' in card\n",
    "            })\n",
    "\n",
    "    if issue_number in card_map[project_number]:\n",
    "        return card_map[project_number][issue_number]\n",
    "\n",
    "    api_path = '/repos/%s/issues/%d' % (repository.github.origin, issue_number)\n",
    "    status_code, headers, issue = filter_github_request(api_path)\n",
    "\n",
    "    column_index = target_column_index if target_column_index is not None else 0\n",
    "    \n",
    "    api_path = '/projects/columns/%s/cards' % columns[column_index]['id']\n",
    "\n",
    "    data = {\n",
    "        'content_id': issue['id'],\n",
    "        'content_type': 'Issue'\n",
    "    }\n",
    "\n",
    "    status_code, headers, card = filter_github_request(api_path, 'POST', data)\n",
    "\n",
    "    card_map[project_number][issue_number] = {\n",
    "        'card_id': card['id'], 'column_id': columns[column_index]['id']\n",
    "    }\n",
    "\n",
    "    return card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the card is in the right column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_issue_column(project_map, column_map, card_map, repository, folder, issue_number, target_column_index):\n",
    "    project_number = init_github_project(project_map, repository)\n",
    "    columns = init_github_columns(column_map, project_number)\n",
    "    target_column_id = None if target_column_index is None else columns[target_column_index]['id']\n",
    "    \n",
    "    card = get_github_card(card_map, repository, project_number, issue_number, target_column_index, columns)\n",
    "\n",
    "    if target_column_id is None or card['column_id'] == target_column_id:\n",
    "        return\n",
    "\n",
    "    api_path = '/projects/columns/cards/%s/moves' % card['card_id']\n",
    "    \n",
    "    data = {\n",
    "        'position': 'top',\n",
    "        'column_id': target_column_id\n",
    "    }\n",
    "\n",
    "    status_code, headers, update_info = filter_github_request(api_path, 'POST', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_issue_columns(repository, file_info, target_column_map):\n",
    "    project_map = get_github_project_map(repository.github.origin)\n",
    "    column_map = {}\n",
    "    card_map = {}\n",
    "\n",
    "    for folder, target_column_index in target_column_map.items():\n",
    "        init_issue(repository, folder, file_info)\n",
    "        issue_map = get_issue_map(repository)\n",
    "        issue_number = issue_map[repository.github.branch][folder]\n",
    "\n",
    "        update_issue_column(\n",
    "            project_map, column_map, card_map, repository,\n",
    "            folder, issue_number, target_column_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_translation_issues(repository, file_info):\n",
    "    issues = []\n",
    "\n",
    "    api_path = '/repos/%s/issues?milestone=%d' % \\\n",
    "        (repository.github.origin, get_milestone_map(repository)['human'])\n",
    "\n",
    "    status_code, headers, issues = filter_github_request_all(api_path)\n",
    "    \n",
    "    target_column_map = {}\n",
    "    \n",
    "    for issue in issues:\n",
    "        branch, folder = issue['title'].split(' - ')\n",
    "        \n",
    "        if branch == repository.github.branch and folder.find(repository.github.project_folder) == 0:\n",
    "            key = '%s/%s' % (branch, folder)\n",
    "            \n",
    "            if key in file_info:\n",
    "                approved = file_info[key]['approved']\n",
    "                phrases = file_info[key]['phrases']\n",
    "\n",
    "                if approved == 0:\n",
    "                    target_column_map[folder] = 0\n",
    "                elif approved == phrases:\n",
    "                    target_column_map[folder] = 2\n",
    "                else:\n",
    "                    target_column_map[folder] = 1\n",
    "            else:\n",
    "                target_column_map[folder] = None\n",
    "    \n",
    "    update_issue_columns(repository, file_info, target_column_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cron job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the list of all files we should translate and all updated files that we need to re-translated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_branch_files(branch):\n",
    "    _git('fetch', 'upstream', '--no-tags', '%s:refs/remotes/upstream/%s' % (branch, branch))\n",
    "\n",
    "    diff_output = _git('diff', '--name-only', '%s..upstream/%s' % (branch, branch))\n",
    "    new_files = get_md_files(diff_output, 'en')\n",
    "\n",
    "    lstree_output = _git('ls-tree', '-r', '--name-only', branch)\n",
    "    all_files = get_md_files(lstree_output, 'en')\n",
    "    \n",
    "    return new_files, all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep CrowdIn and GitHub translations in sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sources(repository, new_files):\n",
    "    _git('checkout', repository.github.branch)\n",
    "    _git('merge', 'upstream/%s' % repository.github.branch)\n",
    "\n",
    "    return crowdin_upload_sources(repository, new_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_translations(repository, all_files):\n",
    "    now = datetime.now()\n",
    "\n",
    "    crowdin_download_translations(repository, all_files)\n",
    "\n",
    "    status_output = '\\n'.join([line[3:] for line in _git('status', '--porcelain').split('\\n')])\n",
    "    commit_files = get_md_files(status_output, 'ja')\n",
    "\n",
    "    if len(commit_files) == 0:\n",
    "        return\n",
    "    \n",
    "    for file in commit_files:\n",
    "        if file[-3:] == '.md':\n",
    "            continue\n",
    "\n",
    "        md_file = file[:-9] + '.md'\n",
    "\n",
    "        if os.path.isfile(md_file):\n",
    "            os.remove(md_file)\n",
    "            os.rename(file, md_file)\n",
    "\n",
    "    status_output = '\\n'.join([line[3:] for line in _git('status', '--porcelain').split('\\n')])\n",
    "    commit_files = get_md_files(status_output, 'ja')\n",
    "\n",
    "    if len(commit_files) == 0:\n",
    "        return    \n",
    "    \n",
    "    _git('add', *commit_files)\n",
    "    _git('commit', '-m', 'Updated translations %s' % now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform any needed GitHub issue maintenance to keep project boards usable, perform any needed CrowdIn file maintenance to stay under the quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_files(repository, file_info):\n",
    "    to_close = set()\n",
    "    to_delete = set()\n",
    "    \n",
    "    prefix = repository.github.branch + '/'\n",
    "    issue_map = get_issue_map(repository)\n",
    "    \n",
    "    for file, metadata in file_info.items():\n",
    "        if file.find(prefix) != 0:\n",
    "            continue\n",
    "        \n",
    "        folder = file[len(prefix):]\n",
    "        \n",
    "        if folder not in issue_map[repository.github.branch]:\n",
    "            continue\n",
    "        \n",
    "        if metadata['phrases'] == metadata['approved']:\n",
    "            to_close.add(file)\n",
    "\n",
    "    for folder in to_close:\n",
    "        prefix = '%s/%s/' % (repository.github.branch, folder)\n",
    "\n",
    "        for file, metadata in file_info.items():\n",
    "            if file.find(prefix) == 0:\n",
    "                to_delete.add(file)\n",
    "\n",
    "    for folder in to_close:\n",
    "        close_issue(repository, folder, file_info)\n",
    "            \n",
    "    for file in to_delete:\n",
    "        print(file)\n",
    "        #del file_info[file]\n",
    "        #delete_translation(repository, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update local copies of translations, translation memory, and glossaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for repository in repositories:\n",
    "    logging.info('cd %s' % repository.github.git_root)\n",
    "    os.chdir(repository.github.git_root)\n",
    "\n",
    "    new_files, all_files = get_branch_files(repository.github.branch)\n",
    "    file_info = update_sources(repository, new_files)\n",
    "\n",
    "    new_translation_needed = []\n",
    "\n",
    "    new_translation_needed.extend(init_issues(repository, all_files, file_info))\n",
    "    new_translation_needed.extend(reopen_issues(repository, new_files, file_info))\n",
    "\n",
    "    if len(new_translation_needed) > 0:\n",
    "        pre_translate(repository, new_translation_needed, file_info)\n",
    "\n",
    "    update_translations(repository, all_files)\n",
    "    cleanup_files(repository, file_info)\n",
    "    update_translation_issues(repository, file_info)\n",
    "\n",
    "    save_translation_memory(repository)\n",
    "    save_glossary(repository)\n",
    "\n",
    "    logging.info('cd -')\n",
    "    os.chdir(initial_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
